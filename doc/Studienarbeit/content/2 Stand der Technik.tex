\chapter{Stand der Technik}
\section{Straßenschilderkennung}
\input{content/Stand der Technik/Strassenschilderkennung.tex}

\section{Künstliche Neuronale Netze}
\input{content/Stand der Technik/KNNs.tex}

\section{Bildgenerierung mit Künstlichen Neuronalen Netzen}
\input{content/Stand der Technik/NoGANs.tex}

%\section{Generative Adversarial Networks}
%\input{content/Stand der Technik/GANs.tex}

\section{Vorherige Arbeiten}
\input{content/Stand der Technik/VorherigeArbeiten.tex}

\section{Machine Learning Frameworks}

\acused{GPU}

Es ist möglich, \acp{KNN} von Grund auf zu programmieren. Die Hauptaufgabe von Entwickelnden besteht jedoch darin, sowohl den Datensatz als auch die Architektur sowie die Hyperparameter des Modells zu entwerfen und anzupassen. Verschiedene \emph{Frameworks} bieten für die Berechnung der Vorhersagen und das Optimieren eines \ac{KNN} bereits Funktionen an. Sie sorgen zudem dafür, dass diese Funktionen möglichst performant sind. Hierfür spielen vor allem Grafikkarten (\acp{GPU}) eine bedeutende Rolle. Neben sogenannten Tensor Processing Units (TPUs) und Field Programmable Arrays (FPGAs) werden nämlich in erster Linie \acp{GPU} für das Berechnen von \acp{KNN} eingesetzt, da sie dafür performanter sind als Prozessoren. \cite{frameworks}

Einige Frameworks im Bereich des maschinellen Lernens unterstützen eine Berechnung auf \acp{GPU}. Dazu zählen unter anderem \textbf{TensorFlow}, \textbf{PyTorch}, \textbf{MXNet}, \textbf{Microsoft CNTK} und \textbf{Caffe}. Eine Veröffentlichung aus dem Jahre 2019 vergleicht dabei mitunter diese Frameworks. Das am meisten verbreitete Framework sei dabei TensorFlow. Es wurde im Jahre 2015 von der Firma Google entwickelt und ist, wie die meisten anderen genannten Frameworks, überwiegend in der Programmiersprache C++ geschrieben. PyTorch stammt von der Firma Facebook und basiert auf dem Framework \emph{Torch}. Einige Frameworks wie Caffe sind für spezielle Anwendungsgebiete gedacht, wohingegen beispielsweise TensorFlow und PyTorch allgemein Anwendung finden. Anwendende können die Funktionen aller genannten Frameworks mit der Sprache Python nutzen, welche als die für das maschinelle Lernen am meisten eingesetzte Programmiersprache gilt. \cite{frameworks}

Eine bestimmte Art und Weise, wie Frameworks \acp{KNN} optimieren ist im Verlauf dieser Arbeit relevant: Viele der genannten Frameworks unterteilen Datensätze in sogenannte \emph{Batches}. Jeder Batch beinhaltet einen Teil des Datensatzes. Eine \emph{Batch Größe} \emph{(engl.: batch size)} legt fest, wie viele Elemente sich in einem Batch befinden. Besitzt der Datensatz $1024$ Bilder und das \ac{KNN} eine Batch Größe von $16$, dann wird der Datensatz in $64$ Batches unterteilt, da $\frac{1024}{16} = 64$. Es ist möglich, alle Elemente eines Batches gleichzeitig in ein neuronales zu speisen. Bei einer Batch Größe von $16$ erhält das \ac{KNN} pro Trainingsschritt $16$ Eingaben und trifft somit eben so viele Vorhersagen. In einem Trainingschritt wird das \ac{KNN} dann auf allen $16$ Vorhersagen trainiert. Die Frameworks sorgen dafür, dass die Batches dynamisch in den Arbeitsspeicher geladen werden. Somit muss der Arbeitsspeicher keine Kapazität für den gesamten Datensatz besitzen. \cite{tf-dataset}

Weiterhin ist mitunter in TensorFlow und PyTorch der Begriff des \emph{Tensors} relevant. Für diese Arbeit kann ein Tensor als ein mehrdimensionales Array betrachtet werden. Tensoren werden mit einer Stufe beschrieben, die ihre Dimensionalität angibt. Ein Skalar hat die Stufe 0, ein Vektor die Stufe 1 und eine Matrix die Stufe 2. Weiterhin besitzen Tensoren eine Form. Ein Tensor dritter Stufe der Form $(256, 256, 3)$ besitzt eine Höhe und Breite von 256 sowie eine Tiefe von 3. Das kann zum Beispiel ein digitales Bild mit den Pixelmaßen 256x256 und drei Farbkanälen sein. Betrachtet man einen Batch solcher Bilder mit einer Batch Größe von 16, dann erhält man einen Tensor der Stufe vier mit der Form $(16, 256, 256, 3)$. Vorstellen kann man sich das als 16 \emph{übereinander gestapelte} Bilder. Würde man nun zwei solcher Batches in einem Tensor zusammenfassen, dann hätte dieser Tensor die Form $(2, 16, 256, 256, 3)$ \cite{learn-tensorflow}.