\chapter{Evaluation}

Die Evaluation soll prüfen, inwieweit die Implementierung dieser Studienarbeit die in Kapitel \ref{chap:ziel-der-arbeit} definierten Ziele erreicht. Somit beurteilt die Evaluation die folgenden Fragestellungen:
\begin{itemize}
	\item Wie \textbf{fotorealistisch} sind die generierten Bilder des Modells? 
	\item Wie \textbf{neuartig} sind die generierten Bilder des Modells?
	\item Wie sehr eignen sich die augmentierten Grenzfälle als \textbf{Trainingsdaten} \\ für Straßenschilderkennungs-Software?
\end{itemize}

Dahingehend kann die Beurteilung in eine \textbf{Evaluation der Generierung} und eine \textbf{Evaluation der Augmentierung} unterteilt werden.

\label{chap:Evaluation}
\section{Evaluation der Generierung}

\subsection{Vorgehen}

Das Modell dieser Studienarbeit verfolgt das Ziel, die statistische Verteilung der Trainingsdaten möglichst gut abzubilden. In dem Trainingsdatensatz befinden sich ausschließlich reale Aufnahmen von Straßenschildern. Daher kann argumentiert werden, dass das Modell genau dann fotorealistische Bilder erzeugt, wenn die generierte Verteilung $\hat{p}(x)$ ähnlich ist zu der tatsächlichen Verteilung der Trainingsdaten $p(x)$. Das Ziel der Evaluierung ist somit ein Vergleich, wie ähnlich sich die beiden Verteilungen $\hat{p}(x)$ und $p(x)$ sind.

In der Beurteilung generativer Modelle sind dafür der \emph{Inception Score} sowie die \emph{Fréchet Inception Distance} üblich. Beide Verfahren nutzen ein Klassifizierungsmodell namens \emph{Inception} und einen Datensatz namens \emph{ImageNet}. Der Datensatz ImageNet enthält 14 Millionen Bilder, die in 20.000 Kategorien eingeteilt sind. Das Klassifizierungsmodell Inception ist ein \ac{CNN} von Google. Sowohl der Inception Score als auch die Fréchet Inception Distance basieren auf folgendem Verfahren:
\begin{itemize}
	\item Ein Inception Modell wird auf dem ImageNet Datensatz trainiert
	\item Das generative Modell wird darauf trainiert, Bilder zu generieren, die dem ImageNet Datensatz ähnlich sehen
	\item Das Inception Modell soll die generierten Bilder klassifizieren
	\item Je höher die Genauigkeit des Inception Modells ist, desto ähnlicher sind die generierten Bilder dem ImageNet Datensatz
\end{itemize}
Die Klassifizierungsgenauigkeit schafft dabei einen quantitativen Wert für die Ähnlichkeit der beiden Verteilungen $\hat{p}(x)$ und $p(x)$. Hiermit können generative Modelle verglichen werden.

Das Verfahren kann für die Evaluierung nicht gewählt werden, da das \ac{CycleGAN} darauf trainiert ist, Bilder von Straßenschildern zu erzeugen. Es wäre notwendig, das Modell zusätzlich auf den ImageNet Datensatz zu trainieren.

Stattdessen verwendet die Evaluation ein adaptiertes Vorgehen, das in Abbildung \ref{fig:eval} dargestellt ist.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../images/Eval/Eval.png}
	\caption{Vorgehen der Evaluierung}
	\label{fig:eval}
\end{figure}

Das Verfahren prüft folgendes: Wie genau kann ein Klassifikator die Testbilder des \ac{GTSRB} klassifizieren, wenn er mit einem bestimmten Datensatz trainiert ist? Der \ac{GTSRB} wird stellvertretend als die Verteilung $p(x)$ der Bilder von deutschen Straßenschildern betrachtet. Je ähnlicher dazu die Verteilung $\hat{p}(x)$ des Datensatzes ist, desto besser kann der Klassifikator daraus lernen, die Bilder des \ac{GTSRB} zu klassifizieren.

Damit ist die Genauigkeit des Klassifikators, wenn er mit den Trainingsdaten des \ac{GTSRB} trainiert wird der \textbf{Referenzwert}. Die Genauigkeit des Klassifikators, wenn er mit dem zu evaluierenden Datensatz trainiert wird ist hingegen der \textbf{Testwert}. Die absolute Differenz zwischen Referenz- und Testwert stellt die Evaluationsmetrik dar. Je geringer die Differenz ist, desto ähnlicher sind die generierten Bilder dem \ac{GTSRB} Datensatz.

Der Klassifikator ist ein VGG16 Modell. Das ist eine \ac{CNN}-Architektur aus dem Jahre 2014, die an der Oxford Universität entwickelt wurde. Ein Inception Klassifikator könnte bei dieser Problemstellung eine zu 

%\begin{table}[H]
%	\centering
%	\begin{tabular}{|l|c|c|c|}
%	\hline
%	Trainingsdatensatz & Trainingsbilder (\#) & Trainingsg.\tablefootnote{Genauigkeit auf den Trainingsdaten} (\%) & Epochen (\#) \\ \hline \hline
%	Präparierter GTSRB & 4.554 & 100 & 20 \\ \hline
%   Gesamte Trainingsdaten & 5.685 & 100 & 10 \\ \hline \hline
%   U-Net & 4.300 & 99,60 & 16 \\ \hline
%	ResNet (6 residual blocks) & 4.300  & 100 & 20 \\ \hline
%   ResNet (9 residual blocks) & 4.300 & 99,33 & 20 \\ \hline \hline
%   Piktogramme & 43 & 100 & 20 \\ \hline
%   Augmentierte Piktogramme & 4.300 & 99,65 & 20
%    \\ \hline \hline
%	Gemischt & & & \\ \hline
%	\end{tabular}
%	\caption{Training eines VGG16 Klassifikators mit unterschiedlichen Trainingsdaten}
%\end{table}

\subsection{Ergebnisse}

Tabelle \ref{tab:results} zeigt die Ergebnisse der Evaluierung.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|c|c|}
	\hline
	Trainingsdatensatz & Trainingsbilder (\#) & Testgenauigkeit (\%) \\ \hline \hline
	Präparierter GTSRB & 4.554 & 82 \\ \hline
   Gesamte Trainingsdaten & 5.685 & 83 \\ \hline \hline
   U-Net & 4.300 & 62 \\ \hline
	ResNet (6 residual blocks) & 4.300 & 46 \\ \hline
   ResNet (9 residual blocks) & 4.300 & 53 \\ \hline \hline
	Augmentierte Piktogramme & 4.300 & 19 \\ \hline
   Piktogramme & 43 & 12 \\ \hline \hline
	Gemischt & 8.868 & \\ \hline
	\end{tabular}
	\caption{Ergebnisse des Trainings eines VGG16 Klassifikators}
	\label{tab:results}
\end{table}

Die Referenzgenauigkeit des \ac{GTSRB} beträgt 82\%. Der gesamte in Kapitel \ref{chap:3-Datensatz} beschriebene Datensatz hat eine vergleichbare Genauigkeit von 83\% auf den Testdaten des \ac{GTSRB}. Das deutet darauf hin, dass die hinzugefügten Datensätze die Verteilung $p(x)$ von deutschen Straßenschildern nicht verzerren.

Die drei Varianten des \ac{CylceGAN} -- U-Net-basiert, ResNet-basiert mit 9 Residual Blocks und ResNet-basiert mit 6 Residual Blocks -- erzeugen Bilder, die zu einer signifikant niedrigeren Genauigkeit führen. Das bedeutet, dass sich die Bilder zu einem gewissen Grad von denen des \ac{GTSRB} unterscheiden. Analog zu den Beobachtungen in Kapitel \ref{chap:trainingsergebnisse} zeigt dabei das U-Net-basierte \ac{CycleGAN} die geringste Abweichung zum Referenzwert von 82\%. Somit trifft auf dieses Modell mindestens eine der beiden Aussagen zu:
\begin{enumerate}
	\item Das U-Net-basierte \ac{CycleGAN} erzeugt \textbf{fotorealistischere} Bilder als die ResNet-basierten Modelle
	\item Das U-Net-basierte \ac{CycleGAN} erzeugt eine \textbf{größere Varianz} an unterschiedlichen Bildern pro Klasse
\end{enumerate}
Von den beiden ResNet-basierten \acp{CycleGAN} zeigt die Variante mit 9 Residual Blocks die höhere Testgenauigkeit.

Das \ac{CycleGAN} übersetzt die Piktogramme aus der Domäne X in die Domäne Y. Eine Fragestellung ist hier, ob die Bilder der Domäne Y eine höhere Genauigkeit erzielen als die rohen Piktogramme. Ist das nicht der Fall, dann würde das zeigen, dass die von dem \ac{CycleGAN} implementierte Funktion keinen Mehrwert bietet. Es wäre ebenso möglich, den Datensatz um Bilder von Piktogrammen zu erweitern.

Hier zeigt sich jedoch ein signifikanter Unterschied. Die augmentierten Piktogramme erzielen eine Testgenauigkeit von 19\%. Der Abstand dieses Datensatzes zum U-Net-basierten Datensatz beträgt 43\%. Der Abstand vom U-Net-basierten Datensatz zum \ac{GTSRB} ist hingegen geringer mit einem Wert von 20\%.

\subsection{Ergründung}
Wie bereits in Kapitel \ref{chap:trainingsergebnisse} erwähnt, erzeugen die drei trainierten Varianten des \ac{CycleGAN} für einige Klassen weniger fotorealistische Bilder als für andere. So zum Beispiel für die vier Arten von Aufhebungsschildern, für die der 5685 Bilder umfassende Trainingsdatensatz lediglich 44 Beispielbilder enthält. Es ist zu erwarten, dass der VGG16-Klassifikator hieraus nicht lernen kann, echte Bilder von Aufhebungsschildern korrekt zu klassifizeren. Diese Kategorie von Straßenschildern macht 9\% aller 43 Klassen aus.

Weitere Eigenheiten der Modelle sind die folgenden:
\begin{itemize}
	\item \textbf{U-Net-basiertes \ac{CycleGAN}}
		\begin{itemize}
			\item Vorfahrtschilder (Klassen-ID 12) zeigen in Epoche 150 weder eine hohe Varianz auf, noch sind die fotorialistisch. Epoche 200 verbessert das 
			\item Richtungsweisende Schilder (Klassen-IDs 33-40) sind nach einer subjektiven Beurteilung die fotorealistischsten Bilder
			\item C
		\end{itemize}
	\item \textbf{ResNet-basiertes \ac{CycleGAN} (9 Residual Blocks)}
		\begin{itemize}
			\item A
			\item B
			\item C
		\end{itemize}
	\item \textbf{ResNet-basiertes \ac{CycleGAN} (6 Residual Blocks)}
		\begin{itemize}
			\item A
			\item B
			\item C
		\end{itemize}
\end{itemize}

\section{Evaluation der Augmentierung}

\section{Verbesserungsmöglichkeiten}
\begin{itemize}
   \item Bestimmte Funktionen in TensorFlow Graphen umwandeln; Dadurch Performance der Implementierung verbessern
   \item Learning Rate Decay
\end{itemize}
